{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIjFqlgERHAq"
   },
   "source": [
    "# IMA-205\n",
    "# TP ANN (part B)\n",
    "\n",
    "## Coding a Multi-Layer Perceptron with Tensorflow\n",
    "Author : Alasdair Newson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG_lwX5GRHAs"
   },
   "source": [
    "In the second part of this TP, we will be looking at the Multi-Layer Perceptron (MLP) using Tensorflow (http://www.tensorflow.org)\n",
    "\n",
    "We will be using the following packages :\n",
    "   \n",
    "   - Scikit-learn (http://scikit-learn.org/)\n",
    "   - Tensorflow (http://www.tensorflow.org)\n",
    "\n",
    "The following commands will make sure that you have all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCGFw05kRHAs"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline                      \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import sklearn  # scikit-learn\n",
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import optimizers\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxgAcMrMRHAt"
   },
   "source": [
    "# 1 - Multi-Layer Perceptron with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjK-B5mkRHAt"
   },
   "source": [
    "## Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzVmdesCRHAu"
   },
   "source": [
    "Tensorflow is an environment written by Google which allows easy implementation of deep neural networks. In particular, it provides automatic differentiation so that the user does not have to determine gradients manually, which can be extremely long even for simple networks, as you have seen in the previous TP*.\n",
    "\n",
    "This, in turn allows a user to apply techniques such as stochastic gradient descent for training purposes. The differentiation graph is created symbolically when the network is created. Since a neural network is simply a cascade (or concatenation) of simple functions, the network in Tensorflow is simply a sequence of functions which are applied to one or several variables. The output of the final function is the output of the network.\n",
    "\n",
    "For instance, if the input were ```x``` and the network consisted of two functions ```a``` and ```b```, you could create the network simply by writing :\n",
    "\n",
    "```y = b(a(x))```\n",
    "\n",
    "However, it is clear that these functions and variables must be of some special type, so that the computer can figure out how to carry out the automatic differentiation for training. Tensorflow provides these functions and variables, which must be created with the Tensorflow package.\n",
    "\n",
    "Previously, Tensorflow required a strict separation between the creation (declaration) of variables and their execution (giving a numerical value, using the ```Session``` function). With Keras and Tensorflow 2, this separation has been removed, and the language has become simpler, in particular for creating and training networks.\n",
    "\n",
    "## Creating a network\n",
    "\n",
    "To create a network, there are two (main) methods :\n",
    "\n",
    "### Creating a model using the Sequential API\n",
    "\n",
    "Tensorflow has a simple way of adding layers to create a neural network. First, you can indicate to Tensorflow that the model is 'sequential' (a simple model, with not many tweaks). For this, you can use the following function :\n",
    "- ```model = Sequential()```\n",
    "\n",
    "After this, you can add layers with the function.\n",
    "\n",
    "- ```model.add()```\n",
    "\n",
    "You can then use the ```Dense``` (and other) functions to specify different layer types.  __Note that in the case of this approach, you will have to specify the input image size in the first layer of the network, inside the first layer function.__ So, for example, if the first layer is a dense layer with a relu activation, with n_out output neurons, and n_input neurons :\n",
    "\n",
    "- model = Sequential()\n",
    "- model.add(Dense(n_out,activation,input_shape=(n_in,),activation='relu'))\n",
    "\n",
    "Otherwise, the network does not know how many weights to create. __Be careful of this special case (the first layer)__ \n",
    "\n",
    "### Creating a model using the standard API\n",
    "\n",
    "Otherwise, another approach to creating the model is to explicitly create the input variable, and just cascade the different functions, as in Tensorflow. So, for the same example, we would have :\n",
    "\n",
    "- input = Input(shape=(n_in,))\n",
    "- output = Dense(n_in,n_out,activation='relu')(input)\n",
    "- model = Model(input, output)\n",
    "\n",
    "\n",
    "For now, let's use the Sequential API (however, if you want to try the standard API, go ahead).\n",
    "\n",
    "## Training and testing a network\n",
    "\n",
    "Tensorflow allows the easy training of a network with the following functions :\n",
    "\n",
    "- model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=learning_rate)) : create the loss function and define the optimiser used. This function can do many other useful things (such as specifying different metrics to look at the model's performance\n",
    "\n",
    "- model.fit(...) : train the model\n",
    "- model.evaluate(...) : test the model\n",
    "- model.predict(...) : carry out a simple forward pass on the model\n",
    "\n",
    "Please look at the Tensorflow documenation for further details on these functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYFxzt5mRHAy"
   },
   "source": [
    "# MLP with TensorFlow\n",
    "\n",
    "We are now going to create an MLP with tensorflow. We will start with an MLP with one hidden layer. The network should consist of the following operations, in the following order :\n",
    "\n",
    "- Fully connected layer, with weights w1 and biases b1\n",
    "- ReLU activation\n",
    "- Fully connected layer, with weights w2 and biases b2\n",
    "- Sigmoid output activation\n",
    "\n",
    "We are going to be solving a binary classification \n",
    "problem, so the output of the network should be a scalar between 0 and 1 (thus the last layer is a Sigmoid activation).\n",
    "\n",
    "The loss function should be defined as the binary cross-entropy between the predicted class and the true class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdGVzE6NRHAy"
   },
   "source": [
    "First, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfiJwuLmRHAy"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, Y = datasets.make_circles(n_samples=1000, noise=0.05, factor=0.5)\n",
    "# X, y = datasets.make_moons(n_samples=1000, random_state=42)  # try with 2 moons\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "#show data in plot\n",
    "plt.plot(X[Y == 1, 0], X[Y == 1, 1], 'ro')\n",
    "plt.plot(X[Y == 0, 0], X[Y == 0, 1], 'bo')\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRL1c6UyRHAy"
   },
   "source": [
    "We are now going to define some useful auxillary functions.\n",
    "\n",
    "First, a function that shows the decision boundary of our network. This works only for 2D input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0cUhY8xRHAz"
   },
   "outputs": [],
   "source": [
    "# THIS CODE IS GIVEN\n",
    "\n",
    "def plot_decision_function_2d(model_mlp, X, Y):\n",
    "   # create a mesh to plot in\n",
    "    h = .02  # step size in the mesh\n",
    "    offset = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - offset, X[:, 0].max() + offset\n",
    "    y_min, y_max = X[:, 1].min() - offset, X[:, 1].max() + offset\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = model_mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    print(Z.shape)\n",
    "    print(Z)\n",
    "    \n",
    "    Z = Z<0.5\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.plot(X[Y == 1, 0], X[Y == 1, 1], 'yo')\n",
    "    plt.plot(X[Y == 0, 0], X[Y == 0, 1], 'ko')\n",
    "\n",
    "    plt.title(\"Decision surface\")\n",
    "    plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKtZJ5iQRHA0"
   },
   "source": [
    "## CREATING AND TRAINING THE MODEL\n",
    "\n",
    "We are now ready to create our network with a Tensorflow Session and to carry out training on our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WO8dHtrGRHA0"
   },
   "outputs": [],
   "source": [
    "# FILL IN CODE BY STUDENTS IN THIS SECTION\n",
    "\n",
    "#We split up the data into training and test data, using a function from Scikit-learn :\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Layer sizes\n",
    "n_input = X.shape[1]  # Number of input features\n",
    "n_hidden = 50  # Number of hidden nodes\n",
    "\n",
    "# create the model (using the sequential API)\n",
    "\n",
    "model_mlp = Sequential([...])   # FILL IN STUDENTS\n",
    "\n",
    "# create the loss and optimiser\n",
    "learning_rate = 0.01\n",
    "model_mlp.compile(loss=..., optimizer=...,metrics=[\"accuracy\"]) # TO FILL IN\n",
    "\n",
    "# Run optimisation algorithm\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "print('Training')\n",
    "model_mlp.fit(..., ..., epochs=...,batch_size=...) # TO FILL IN\n",
    "\n",
    "print('Testing')\n",
    "model_mlp.evaluate(...,  ..., verbose=2) # TO FILL IN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpBaU1rcc0oY"
   },
   "outputs": [],
   "source": [
    "plot_decision_function_2d(model_mlp, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oni6Q5vRHA0"
   },
   "source": [
    "# MLP with several hidden layers\n",
    "\n",
    "We are now going to create an MLP with several hidden layers. We are going to use a more complicated dataset : the MNIST dataset, which contains images of handwritten digits. There are 10 classes, one for each digit.\n",
    "\n",
    "We are going to implement the following architecture :\n",
    "\n",
    "- Fully connected layer\n",
    "- Relu activation\n",
    "- Fully connected layer\n",
    "- Relu activation\n",
    "- Fully connected layer\n",
    "- Relu activation\n",
    "- Fully connected layer\n",
    "- Softmax output activation\n",
    "\n",
    "__IMPORTANT Note__ The sigmoid layer has been replaced by a softmax layer (at the end). This is normal, since we have a multi-class problem.\n",
    "\n",
    "\n",
    "First, we load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0cGLMKRRHA0"
   },
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(y_train)  # in order to convert y to a matrix with (num_examples, num_classes) (one-hot encoding)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test)  # in order to convert y to a matrix with (num_examples, num_classes) (one-hot encoding)\n",
    "\n",
    "#reshape the input images : flatten the last two dimensions\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzMV_9-NRHA1"
   },
   "source": [
    "Finally, fill in the following code to create and train your MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G7AToqyRHA1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_hidden_3 = 128 # 2nd layer number of neurons\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_classes = Y_train.shape[1] # MNIST total classes (0-9 digits)\n",
    "\n",
    "# TO CODE BY STUDENTS\n",
    "\n",
    "\n",
    "model_mlp_multi_layer = ...\n",
    "\n",
    "# create the loss and optimiser, use 'categorical_crossentropy' in loss\n",
    "...\n",
    "\n",
    "# Run optimisation algorithm\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "print('Training')\n",
    "...\n",
    "\n",
    "print('Testing')\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6Q-JntxluxG"
   },
   "outputs": [],
   "source": [
    "# THIS CODE IS GIVEN\n",
    "\n",
    "def test_mnist_images(model_mlp_multi_layer,X_test):\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  for idx in range(0,10):\n",
    "      plt.subplot(2, 5, idx+1)\n",
    "      rand_ind = np.random.randint(0,X_test.shape[0])\n",
    "      plt.imshow(np.reshape(X_test[rand_ind,:],(28,28)),cmap='gray')\n",
    "      # get prediction\n",
    "      model_prediction = np.argmax(model_mlp_multi_layer.predict(np.expand_dims( X_test[rand_ind,:], axis=0)),axis=1)\n",
    "      plt.title(int(model_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2D9RE4apVWW"
   },
   "outputs": [],
   "source": [
    "test_mnist_images(model_mlp_multi_layer,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lItdBVnfu7xK"
   },
   "source": [
    "You may observe some difficulties in getting good performance in this case. To improve training, we can turn to __regularisation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgao_U5XRHA2"
   },
   "source": [
    "## BATCH NORMALISATION\n",
    "\n",
    "One approach to regularisation which we have seen during the lesson is known as batch normalisation, which we have seen during the lesson. This can be implemented very simply in Tensorflow by adding the following layer :\n",
    "\n",
    "- ```BatchNormalization()```\n",
    "\n",
    "Change your model below, and implement this using your code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ela5-4XRHA2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "# TO CODE BY STUDENTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSxGzvQ3pg3u"
   },
   "outputs": [],
   "source": [
    "test_mnist_images(model_mlp_multi_layer,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp2VIfZ1v7lN"
   },
   "source": [
    "Question : do you observe improved convergence ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQPJDop31Nci"
   },
   "source": [
    "## Dropout\n",
    "\n",
    "You may see that there is a difference between the training accuracy and the testing accuracy. This is the problem of overfitting. To alleviate this problem, we can turn to _dropout_:\n",
    "\n",
    "```tf.keras.layers.Dropout(rate,...)```\n",
    "\n",
    "where rate is the probability that an input neuron to a layer will get set to 0. You can do this in any layer. Use the previous architecture and try out the droupout. Set the dropout rate to 0.1 as a first test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-BF-fpo1PpW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "\n",
    "# TO CODE BY STUDENTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2CWgKZeRHA2"
   },
   "source": [
    "#### Documentation:\n",
    "\n",
    "  - http://www.tensorflow.org\n",
    "  - http://www.deeplearningbook.org/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP_ann_mlp_part_2_students.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
